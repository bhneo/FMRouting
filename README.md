# FMRouting
This repository offers TensorFlow implementations for the paper "An Efficient Agreement Mechanism in CapsNet By
Pairwise Product"

## Requirements
* Python 3.6
* tensorflow-datasets 1.0.2
* tensorflow-gpu 2.0.0b1

## Experiments
### Section 4.1
#### Datasets
 cropped-SVHN, Fashion-MNIST, CIFAR10
 
 These benchmark datasets will be downloaded and prepared automatically when running experiments.
#### Replication 
 The results in Figure 5 can be replicated by running:
```bash
python run_4_1.py --routing=dynamic
python run_4_1.py --routing=FM
```
The results of dynamic routing and FM agreement will be saved in log/fashion_mnist, log/cifar10, log/svhn_cropped in **Tensorboard** format,
and the accuracy will also be printed at the end of every epoch.

### Section 4.2.1
#### Datasets
CIFAR10
 
 The dataset will be downloaded and prepared automatically when running experiments.
#### Replication 
 The results in Table 2 can be replicated by running:
```bash
python run_4_2_1.py 
```
The results will be saved in log/cifar10 in **Tensorboard** format,
and the accuracy will also be printed at the end of every epoch.
### Section 4.2.2
#### Datasets
 cropped-SVHN, Fashion-MNIST, CIFAR10
 
 These benchmark datasets will be downloaded and prepared automatically when running experiments.
#### Replication 
 The results in Table 3 can be replicated by running:
```bash
python run_4_2_2.py 
```
The results will be saved in log/cifar10 in **Tensorboard** format,
and the accuracy will also be printed at the end of every epoch.
### Section 4.3.1
#### Datasets
 ExpandedMNIST, affNIST
 
 This experiment validates the robustness of the network on affine transformations.
It uses the same datasets in the section 5.2 of [Dynamic Routing Between Capsules](https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf)
, ExpandedMNIST is generated using the code in its [official repository](https://github.com/bhneo/models/tree/master/research/capsules),
affNIST can be downloaded in its official [web](https://www.cs.toronto.edu/~tijmen/affNIST/32x/transformed/), we use its
transformed test set as the this experiment's test set.
#### Replication 
 The results in Table 4 can be replicated by running:
```bash
python run_4_3_1.py 
```
The results will be saved in log/aff_mnist in **Tensorboard** format, and the generated digits shown in Table 5
can be found under the Tab "IMAGES" in the **Tensorboard** page.

### Section 4.3.2
#### Datasets
 MultiMNIST
 
 It uses the same dataset in the section 6 of [Dynamic Routing Between Capsules](https://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf)
, MultiMNIST is generated using the code in its [official repository](https://github.com/bhneo/models/tree/master/research/capsules).
#### Replication 
 The results in Table 6 can be replicated by running:
```bash
python run_4_3_2.py 
```
The results will be saved in log/multi_mnist in **Tensorboard** format, and the generated digits shown in Table 6
can be found under the Tab "IMAGES" in the **Tensorboard** page.
### Section 4.4
#### Datasets
 SmallNORB
 
 It uses the same dataset in the section 5 of [Matrix Capsules with EM Routing](https://openreview.net/pdf?id=HJWLfGWRb)
. 
As the official code of "Matrix Capsules with EM Routing" is not ready, we use the "tfrecord" format of SmallNORB generated by the code in 
[this repository]( https://github.com/IBM/matrix-capsules-with-em-routing).
 

 
#### Replication 
 The results in Table 7 can be replicated by running:
```bash
python run_4_4.py 
```
The results will be saved in log/smallNORB in **Tensorboard** format,
and the accuracy will also be printed at the end of every epoch.
